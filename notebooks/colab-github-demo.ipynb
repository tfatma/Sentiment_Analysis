{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tfatma/Sentiment_Analysis/blob/main/notebooks/colab-github-demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pVhOfzLx9us"
      },
      "source": [
        "# Using Google Colab with GitHub\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKJ4bd5rt1wy"
      },
      "source": [
        "\n",
        "[Google Colaboratory](http://colab.research.google.com) is designed to integrate cleanly with GitHub, allowing both loading notebooks from github and saving notebooks to github."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-NVg7RjyeTk"
      },
      "source": [
        "## Loading Public Notebooks Directly from GitHub\n",
        "\n",
        "Colab can load public github notebooks directly, with no required authorization step.\n",
        "\n",
        "For example, consider the notebook at this address: https://github.com/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb.\n",
        "\n",
        "The direct colab link to this notebook is: https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb.\n",
        "\n",
        "To generate such links in one click, you can use the [Open in Colab](https://chrome.google.com/webstore/detail/open-in-colab/iogfkhleblhcpcekbiedikdehleodpjo) Chrome extension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzIRIt9d2huC"
      },
      "source": [
        "## Browsing GitHub Repositories from Colab\n",
        "\n",
        "Colab also supports special URLs that link directly to a GitHub browser for any user/organization, repository, or branch. For example:\n",
        "\n",
        "- http://colab.research.google.com/github will give you a general github browser, where you can search for any github organization or username.\n",
        "- http://colab.research.google.com/github/googlecolab/ will open the repository browser for the ``googlecolab`` organization. Replace ``googlecolab`` with any other github org or user to see their repositories.\n",
        "- http://colab.research.google.com/github/googlecolab/colabtools/ will let you browse the main branch of the ``colabtools`` repository within the ``googlecolab`` organization. Substitute any user/org and repository to see its contents.\n",
        "- http://colab.research.google.com/github/googlecolab/colabtools/blob/main will let you browse ``main`` branch of the ``colabtools`` repository within the ``googlecolab`` organization. (don't forget the ``blob`` here!) You can specify any valid branch for any valid repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmai0dD30XzL"
      },
      "source": [
        "## Loading Private Notebooks\n",
        "\n",
        "Loading a notebook from a private GitHub repository is possible, but requires an additional step to allow Colab to access your files.\n",
        "Do the following:\n",
        "\n",
        "1. Navigate to http://colab.research.google.com/github.\n",
        "2. Click the \"Include Private Repos\" checkbox.\n",
        "3. In the popup window, sign-in to your Github account and authorize Colab to read the private files.\n",
        "4. Your private repositories and notebooks will now be available via the github navigation pane."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J3NBxtZpPcK"
      },
      "source": [
        "## Saving Notebooks To GitHub or Drive\n",
        "\n",
        "Any time you open a GitHub hosted notebook in Colab, it opens a new editable view of the notebook. You can run and modify the notebook without worrying about overwriting the source.\n",
        "\n",
        "If you would like to save your changes from within Colab, you can use the File menu to save the modified notebook either to Google Drive or back to GitHub. Choose **File→Save a copy in Drive** or **File→Save a copy to GitHub** and follow the resulting prompts. To save a Colab notebook to GitHub requires giving Colab permission to push the commit to your repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QAWNjizy_3O"
      },
      "source": [
        "## Open In Colab Badge\n",
        "\n",
        "Anybody can open a copy of any github-hosted notebook within Colab. To make it easier to give people access to live views of GitHub-hosted notebooks,\n",
        "colab provides a [shields.io](http://shields.io/)-style badge, which appears as follows:\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "\n",
        "The markdown for the above badge is the following:\n",
        "\n",
        "```markdown\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "```\n",
        "\n",
        "The HTML equivalent is:\n",
        "\n",
        "```HTML\n",
        "<a href=\"https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "```\n",
        "\n",
        "Remember to replace the notebook URL in this template with the notebook you want to link to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VQqVi-3ScBC",
        "outputId": "f69721ee-4d9e-4cc5-a16f-9e2e27ddcc3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Sentiment_Analysis'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "remote: Total 69 (delta 1), reused 69 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (69/69), 99.92 KiB | 229.00 KiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ],
      "source": [
        "# Replace 'YOUR_GITHUB_REPO_URL' with the URL of your GitHub repository\n",
        "!git clone https://github.com/tfatma/Sentiment_Analysis.git\n",
        "\n",
        "# After cloning, you can navigate into the repository directory\n",
        "# and run your Python file using the !python command\n",
        "\n",
        "# Example:\n",
        "# %cd YOUR_REPO_NAME # Replace YOUR_REPO_NAME with the name of the cloned repository directory\n",
        "# !python your_python_file.py # Replace your_python_file.py with the name of your Python file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC1Jh0-tWTPv",
        "outputId": "191d8e18-c6f4-4931-cc82-a1084214f1f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  Sentiment_Analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd Sentiment_Analysis/"
      ],
      "metadata": {
        "id": "odEUGD6sZ6PB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZhvfKm2Z9fk",
        "outputId": "8cef5606-d08f-47a2-b006-2810f3402a19"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  Sentiment_Analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd Sentiment_Analysis/Project1_NLP/nlp-project/"
      ],
      "metadata": {
        "id": "-nBGz_azaA0s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbX0wBfFaQsG",
        "outputId": "2fb3b677-8006-4b8b-984a-c2e548835184"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  Sentiment_Analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls Sentiment_Analysis/Project1_NLP/nlp-project/scripts/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgaHDUosbSqA",
        "outputId": "cb0e2dff-1390-4391-809c-a2bc69dbc422"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluate_models.py  prepare_data.py\ttest_setup.py\t   train_lora.py\n",
            "minimal_demo.py     run_experiments.py\ttrain_baseline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python  Sentiment_Analysis/Project1_NLP/nlp-project/scripts/train_baseline.py --experiment_name baseline_roberta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeeqbT85b7Fw",
        "outputId": "d091e792-2f4b-4392-9dc9-d69618652a33"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Baseline Model Training: baseline_roberta\n",
            "============================================================\n",
            "\n",
            "Loading configurations...\n",
            "⚠️ Model config not found, using defaults\n",
            "⚠️ Training config not found, using defaults\n",
            "✅ Output directory: outputs/baseline_roberta\n",
            "✅ Random seed set to: 42\n",
            "\n",
            "============================================================\n",
            "DATA PREPARATION\n",
            "============================================================\n",
            "tokenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 165kB/s]\n",
            "config.json: 100% 481/481 [00:00<00:00, 2.89MB/s]\n",
            "vocab.json: 100% 899k/899k [00:00<00:00, 1.98MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 21.9MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 3.25MB/s]\n",
            "\n",
            "============================================================\n",
            "Creating Multi-Domain Sentiment Datasets\n",
            "============================================================\n",
            "Loading Amazon reviews for domains: ['electronics', 'books']...\n",
            "   Attempting amazon_polarity dataset...\n",
            "README.md: 6.81kB [00:00, 23.0MB/s]\n",
            "amazon_polarity/train-00000-of-00004.par(…): 100% 260M/260M [00:07<00:00, 34.5MB/s]\n",
            "amazon_polarity/train-00001-of-00004.par(…): 100% 258M/258M [00:08<00:00, 31.0MB/s]\n",
            "amazon_polarity/train-00002-of-00004.par(…): 100% 255M/255M [00:08<00:00, 28.8MB/s]\n",
            "amazon_polarity/train-00003-of-00004.par(…): 100% 254M/254M [00:09<00:00, 26.3MB/s]\n",
            "amazon_polarity/test-00000-of-00001.parq(…): 100% 117M/117M [00:01<00:00, 67.1MB/s]\n",
            "Generating train split: 100% 3600000/3600000 [00:13<00:00, 270196.64 examples/s]\n",
            "Generating test split: 100% 400000/400000 [00:00<00:00, 456376.47 examples/s]\n",
            "   ✅ Successfully loaded 3600000 total samples\n",
            "   ✅ Processed 140 Amazon samples\n",
            "   Distributed across domains: ['electronics', 'books']\n",
            "Loading IMDb reviews (70 samples)...\n",
            "README.md: 7.81kB [00:00, 33.8MB/s]\n",
            "plain_text/train-00000-of-00001.parquet: 100% 21.0M/21.0M [00:00<00:00, 41.3MB/s]\n",
            "plain_text/test-00000-of-00001.parquet: 100% 20.5M/20.5M [00:00<00:00, 33.5MB/s]\n",
            "plain_text/unsupervised-00000-of-00001.p(…): 100% 42.0M/42.0M [00:00<00:00, 66.2MB/s]\n",
            "Generating train split: 100% 25000/25000 [00:00<00:00, 165105.90 examples/s]\n",
            "Generating test split: 100% 25000/25000 [00:00<00:00, 168618.22 examples/s]\n",
            "Generating unsupervised split: 100% 50000/50000 [00:00<00:00, 172455.92 examples/s]\n",
            "   ✅ Loaded 70 IMDb samples\n",
            "Loading Yelp reviews (70 samples)...\n",
            "   Downloading yelp_polarity dataset...\n",
            "README.md: 8.93kB [00:00, 28.7MB/s]\n",
            "plain_text/train-00000-of-00001.parquet: 100% 256M/256M [00:05<00:00, 44.3MB/s]\n",
            "plain_text/test-00000-of-00001.parquet: 100% 17.7M/17.7M [00:00<00:00, 20.7MB/s]\n",
            "Generating train split: 100% 560000/560000 [00:01<00:00, 280111.30 examples/s]\n",
            "Generating test split: 100% 38000/38000 [00:00<00:00, 308639.41 examples/s]\n",
            "   ✅ Successfully loaded 560000 total samples\n",
            "   ✅ Processed 70 Yelp samples\n",
            "\n",
            "============================================================\n",
            "✅ Dataset Creation Complete!\n",
            "   Train: 200 samples\n",
            "   Val: 40 samples\n",
            "   Test: 40 samples\n",
            "============================================================\n",
            "\n",
            "\n",
            "✅ Datasets created successfully!\n",
            "Data loaded - Train: 200, Val: 40, Test: 40\n",
            "\n",
            "============================================================\n",
            "MODEL INITIALIZATION\n",
            "============================================================\n",
            "Loading model: roberta-base\n",
            "2025-10-20 11:51:32.968542: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760961093.202589     704 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760961093.207620     704 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760961093.220272     704 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760961093.220295     704 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760961093.220299     704 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760961093.220305     704 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-20 11:51:33.224356: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 499M/499M [00:05<00:00, 86.6MB/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Total Parameters:     124,647,939 (124.6M)\n",
            "Trainable Parameters: 124,647,939 (124.65M)\n",
            "Trainable Percentage: 100.00%\n",
            "Model Size:           475.5 MB\n",
            "\n",
            "============================================================\n",
            "TRAINING\n",
            "============================================================\n",
            "Using device: cuda\n",
            "\n",
            "Training Configuration:\n",
            "  Epochs: 1\n",
            "  Learning Rate: 2e-05\n",
            "  Batch Size: 8\n",
            "  Weight Decay: 0.01\n",
            "\n",
            "Starting training...\n",
            "\n",
            "============================================================\n",
            "Starting Training\n",
            "============================================================\n",
            "Epochs: 1\n",
            "Learning Rate: 2e-05\n",
            "Batch Size: 8\n",
            "Training Steps per Epoch: 25\n",
            "============================================================\n",
            "\n",
            "\n",
            "Epoch 1/1\n",
            "------------------------------------------------------------\n",
            "Training: 100% 25/25 [00:06<00:00,  3.96it/s, loss=0.7870, acc=0.4900]\n",
            "Validation: 100% 5/5 [00:00<00:00, 18.79it/s, loss=0.7915, acc=0.6000]\n",
            "\n",
            "📊 Epoch 1 Summary:\n",
            "   Train Loss: 0.9798 | Train Acc: 0.4900\n",
            "   Val Loss:   0.8696 | Val Acc:   0.6000\n",
            "   Learning Rate: 0.00e+00\n",
            "   ✅ New best model! Val Acc: 0.6000\n",
            "------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "Training Complete!\n",
            "============================================================\n",
            "Best Validation Accuracy: 0.6000\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "SAVING RESULTS\n",
            "============================================================\n",
            "✅ Training history saved to outputs/baseline_roberta/training_history.json\n",
            "✅ Final model saved to outputs/baseline_roberta/final_model.pt\n",
            "\n",
            "============================================================\n",
            "FINAL EVALUATION\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Evaluating on Test Set\n",
            "============================================================\n",
            "\n",
            "Testing: 100% 5/5 [00:00<00:00, 11.56it/s]\n",
            "\n",
            "📊 Test Results:\n",
            "   Test Loss: 0.9807\n",
            "   Test Accuracy: 0.4250\n",
            "============================================================\n",
            "\n",
            "✅ Test results saved to outputs/baseline_roberta/test_results.json\n",
            "\n",
            "============================================================\n",
            "🎉 TRAINING COMPLETE!\n",
            "============================================================\n",
            "Results saved to outputs/baseline_roberta\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python Sentiment_Analysis/Project1_NLP/nlp-project/scripts/train_lora.py --experiment_name lora_roberta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCA-XxKqb_zY",
        "outputId": "f04ffe88-a35f-4b8c-d1d6-0bdef415548a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "🚀 LoRA Training: lora_roberta\n",
            "============================================================\n",
            "\n",
            "2025-10-20 12:22:01.204825: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760962921.225613    9093 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760962921.233548    9093 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760962921.248884    9093 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760962921.248909    9093 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760962921.248913    9093 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760962921.248917    9093 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-20 12:22:01.253522: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "✅ PEFT library found\n",
            "\n",
            "============================================================\n",
            "📊 DATA PREPARATION\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "Creating Multi-Domain Sentiment Datasets\n",
            "============================================================\n",
            "Loading Amazon reviews for domains: ['electronics', 'books']...\n",
            "   Attempting amazon_polarity dataset...\n",
            "   ✅ Successfully loaded 3600000 total samples\n",
            "   ✅ Processed 140 Amazon samples\n",
            "   Distributed across domains: ['electronics', 'books']\n",
            "Loading IMDb reviews (70 samples)...\n",
            "   ✅ Loaded 70 IMDb samples\n",
            "Loading Yelp reviews (70 samples)...\n",
            "   Downloading yelp_polarity dataset...\n",
            "   ✅ Successfully loaded 560000 total samples\n",
            "   ✅ Processed 70 Yelp samples\n",
            "\n",
            "============================================================\n",
            "✅ Dataset Creation Complete!\n",
            "   Train: 200 samples\n",
            "   Val: 40 samples\n",
            "   Test: 40 samples\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "🤖 LORA MODEL\n",
            "============================================================\n",
            "\n",
            "Loading base model: roberta-base\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Applying LoRA adapters...\n",
            "✅ LoRA model created successfully!\n",
            "\n",
            "📊 LoRA Parameter Efficiency:\n",
            "   Total parameters: 125,237,763 (125.2M)\n",
            "   Trainable parameters: 592,131 (0.59M)\n",
            "   Trainable percentage: 0.47%\n",
            "   🎯 Training 211.5x fewer parameters!\n",
            "\n",
            "Using device: cuda\n",
            "\n",
            "\n",
            "============================================================\n",
            "Starting Training\n",
            "============================================================\n",
            "Epochs: 2\n",
            "Learning Rate: 0.0003\n",
            "============================================================\n",
            "\n",
            "\n",
            "Epoch 1/2\n",
            "------------------------------------------------------------\n",
            "Training: 100% 25/25 [00:03<00:00,  6.43it/s, loss=0.8308, acc=0.3900]\n",
            "Validation: 100% 5/5 [00:00<00:00, 17.49it/s]\n",
            "\n",
            "📊 Epoch 1 Summary:\n",
            "   Train Loss: 1.0322 | Train Acc: 0.3900\n",
            "   Val Loss:   0.8563 | Val Acc:   0.6000\n",
            "   ✅ New best model! Val Acc: 0.6000\n",
            "\n",
            "Epoch 2/2\n",
            "------------------------------------------------------------\n",
            "Training: 100% 25/25 [00:02<00:00,  8.51it/s, loss=0.8140, acc=0.4700]\n",
            "Validation: 100% 5/5 [00:00<00:00, 17.22it/s]\n",
            "\n",
            "📊 Epoch 2 Summary:\n",
            "   Train Loss: 0.9529 | Train Acc: 0.4700\n",
            "   Val Loss:   0.8591 | Val Acc:   0.7000\n",
            "   ✅ New best model! Val Acc: 0.7000\n",
            "\n",
            "============================================================\n",
            "Training Complete!\n",
            "Best Val Accuracy: 0.7000\n",
            "============================================================\n",
            "\n",
            "\n",
            "✅ Results saved to: outputs/lora_roberta\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python Sentiment_Analysis/Project1_NLP/nlp-project/scripts/compare_models.py\\\n",
        "  --baseline_dir outputs/baseline_roberta \\\n",
        "  --lora_dir outputs/lora_roberta \\\n",
        "  --output_dir outputs/comparison"
      ],
      "metadata": {
        "id": "JSLKJH63k35B",
        "outputId": "8882854e-1f79-4f79-96e8-4b2c9b524719",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "🔍 MODEL COMPARISON ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Baseline directory: outputs/baseline_roberta\n",
            "LoRA directory:     outputs/lora_roberta\n",
            "Output directory:   outputs/comparison\n",
            "\n",
            "Loading results...\n",
            "✅ Results loaded successfully!\n",
            "\n",
            "\n",
            "================================================================================\n",
            "📊 BASELINE vs LoRA COMPARISON\n",
            "================================================================================\n",
            "\n",
            "🎯 PERFORMANCE METRICS:\n",
            "--------------------------------------------------------------------------------\n",
            "Metric                              Baseline             LoRA                 Difference     \n",
            "--------------------------------------------------------------------------------\n",
            "Final Training Accuracy             0.4900               0.4700               -0.0200\n",
            "Final Validation Accuracy           0.6000               0.7000               +0.1000\n",
            "Performance Retention               100%                 116.67              % +16.67%\n",
            "Test Accuracy                       0.4250               N/A                  -              \n",
            "\n",
            "Loss Metrics:                      \n",
            "--------------------------------------------------------------------------------\n",
            "Final Training Loss                 0.9798               0.9529              \n",
            "Final Validation Loss               0.8696               0.8591              \n",
            "\n",
            "⚡ EFFICIENCY METRICS:\n",
            "--------------------------------------------------------------------------------\n",
            "Metric                              Baseline             LoRA                 Improvement    \n",
            "--------------------------------------------------------------------------------\n",
            "Number of Epochs                    1                    2                   \n",
            "LoRA Training Time                  -                    28.66               s\n",
            "LoRA Training Time                  -                    0.48                min\n",
            "\n",
            "Parameter Efficiency:              \n",
            "--------------------------------------------------------------------------------\n",
            "Total Parameters                    125,000,000 (125.0M)\n",
            "Baseline Trainable                  125,000,000 (100%)\n",
            "LoRA Trainable                      2,625,000 (2.11%)\n",
            "Parameter Reduction                                      47.6x fewer\n",
            "\n",
            "📋 CONFIGURATION:\n",
            "--------------------------------------------------------------------------------\n",
            "LoRA Rank (r)                       -                    16                  \n",
            "LoRA Alpha                          -                    32                  \n",
            "Learning Rate                       2e-05                0.0003              \n",
            "Batch Size                          8                    8                   \n",
            "Max Sequence Length                 128                  128                 \n",
            "\n",
            "================================================================================\n",
            "\n",
            "💡 KEY INSIGHTS:\n",
            "--------------------------------------------------------------------------------\n",
            "✓ LoRA MATCHES or EXCEEDS baseline performance (116.7%)\n",
            "✓ LoRA reduces trainable parameters by ~97.9% (47.5x fewer)\n",
            "✓ LoRA training completed in 0.5 minutes\n",
            "✓ LoRA uses higher learning rate (3e-4 vs 2e-5) - typical for adapter tuning\n",
            "✓ Perfect for production: minimal memory footprint, fast training\n",
            "\n",
            "================================================================================\n",
            "\n",
            "✅ Comparison plot saved to: outputs/comparison/comparison_plot.png\n",
            "Figure(1400x1000)\n",
            "✅ Detailed report saved to: outputs/comparison/detailed_comparison_report.json\n",
            "\n",
            "================================================================================\n",
            "✅ COMPARISON COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "All results saved to: outputs/comparison\n",
            "\n",
            "🎤 Interview Talking Points:\n",
            "--------------------------------------------------------------------------------\n",
            "1. 'LoRA achieves comparable performance (70% val acc) with 47x fewer parameters'\n",
            "2. 'Training completed in under 30 seconds with parameter-efficient fine-tuning'\n",
            "3. 'Trade-off: 2% of parameters trained, maintaining competitive accuracy'\n",
            "4. 'Production benefit: Faster iterations, lower deployment costs'\n",
            "5. 'Used rank-16 LoRA adapters on attention layers (query, value, key, dense)'\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In Colab\n",
        "import json\n",
        "\n",
        "# Check baseline results\n",
        "with open('outputs/baseline_roberta/training_history.json', 'r') as f:\n",
        "    baseline = json.load(f)\n",
        "    print(\"Baseline keys:\", baseline.keys())\n",
        "    print(\"\\nBaseline content:\")\n",
        "    print(json.dumps(baseline, indent=2))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Check LoRA results\n",
        "with open('outputs/lora_roberta/results.json', 'r') as f:\n",
        "    lora = json.load(f)\n",
        "    print(\"LoRA keys:\", lora.keys())\n",
        "    print(\"\\nLoRA content:\")\n",
        "    print(json.dumps(lora, indent=2))"
      ],
      "metadata": {
        "id": "P8nZA2ienoem",
        "outputId": "e5f90a7d-b758-4e58-e791-815ac27813d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline keys: dict_keys(['train_loss', 'train_acc', 'val_loss', 'val_acc', 'learning_rates'])\n",
            "\n",
            "Baseline content:\n",
            "{\n",
            "  \"train_loss\": [\n",
            "    0.9798436045646668\n",
            "  ],\n",
            "  \"train_acc\": [\n",
            "    0.49\n",
            "  ],\n",
            "  \"val_loss\": [\n",
            "    0.8695545077323914\n",
            "  ],\n",
            "  \"val_acc\": [\n",
            "    0.6\n",
            "  ],\n",
            "  \"learning_rates\": [\n",
            "    0.0\n",
            "  ]\n",
            "}\n",
            "\n",
            "================================================================================\n",
            "\n",
            "LoRA keys: dict_keys(['config', 'history', 'training_time'])\n",
            "\n",
            "LoRA content:\n",
            "{\n",
            "  \"config\": {\n",
            "    \"model_name\": \"roberta-base\",\n",
            "    \"num_labels\": 3,\n",
            "    \"max_length\": 128,\n",
            "    \"domains\": [\n",
            "      \"electronics\",\n",
            "      \"books\",\n",
            "      \"movies\",\n",
            "      \"restaurants\"\n",
            "    ],\n",
            "    \"train_size\": 50,\n",
            "    \"val_size\": 10,\n",
            "    \"test_size\": 10,\n",
            "    \"batch_size\": 8,\n",
            "    \"num_epochs\": 2,\n",
            "    \"learning_rate\": 0.0003,\n",
            "    \"lora_r\": 16,\n",
            "    \"lora_alpha\": 32,\n",
            "    \"lora_dropout\": 0.1\n",
            "  },\n",
            "  \"history\": {\n",
            "    \"train_loss\": [\n",
            "      1.0321984958648682,\n",
            "      0.9528840923309326\n",
            "    ],\n",
            "    \"train_acc\": [\n",
            "      0.39,\n",
            "      0.47\n",
            "    ],\n",
            "    \"val_loss\": [\n",
            "      0.8562890410423278,\n",
            "      0.8590751528739929\n",
            "    ],\n",
            "    \"val_acc\": [\n",
            "      0.6,\n",
            "      0.7\n",
            "    ]\n",
            "  },\n",
            "  \"training_time\": 28.66396927833557\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "or2DdEvLoB15"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "colab-github-demo.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}